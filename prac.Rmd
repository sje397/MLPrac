Practical Machine Learning - Project
====================================

Make results reproducible. Also enable parallelism to make training faster.

```{r}
set.seed(397)

library(doMC)
registerDoMC(cores = 4)
```

Load the data from the internet. We treat empty strings as 'NA' values.

```{r}
rawTraining <- read.csv(url("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"), na.strings = c("","NA"))
rawTesting <- read.csv(url("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"), na.strings = c("","NA"))
```

We'll just use columns that have no NA values.

```{r}
validCols <- names(rawTraining)[colSums(is.na(rawTraining))==0]
validCols
```

Remove the index, the timestamps and window data since these are not useful as predictors. This is our clean data.

```{r}
validCols <- validCols[8:60]
clean <- rawTraining[,validCols]
```

Separate into training, cross-validation, and test data.

```{r}
library(caret)
inTrain <- createDataPartition(clean$classe, p = 0.75, list = FALSE)
training <- clean[inTrain,]
crossVal <- clean[-inTrain,]
testing <- rawTesting[,validCols[-53]]
```

Train using the Random Forest technique. This will automatically resample (i.e. bagging) and 'boost' using multiple trees. Decision trees are a good model for this problem, as there are likely to be 'thresholds' for many of the accelerometer and gyroscope readings that indicate the quality of the excercise - i.e. moving too quickly or too far will be detectable as a value that exceeds a threshold. A decision tree is able to encode a nested set of such thresholds using the necessary readings.

```{r}
modelFit <- train(classe ~ ., method = "rf", data = training)
modelFit
modelFit$finalModel
```

Out of Sample Error Estimate
----------------------------
By checking against the cross-validation data, we can estimate out of sample error.

```{r}
pred = predict(modelFit, crossVal[,-53])
confusionMatrix(pred, crossVal$classe)
```

We can expect, with 95% confidence, to get an accuracy on the test data between 99.0% and 99.5%.

Thanks for reading!  
Scott

